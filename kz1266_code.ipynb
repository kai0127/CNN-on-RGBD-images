{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdbc9b4a-f656-4200-af0a-aaa85689888c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6daa4-a8b5-4305-94ae-c1a2bf2314fe",
   "metadata": {},
   "source": [
    "&copy; 2022 Kaiwen Zhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73e03b42-3b96-4b17-a7a7-06e301c309bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import cv2\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ea1150-c1e7-466f-ac77-7f7942a26b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def250c1-7d21-4a6d-a1fa-3b7dd863e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_depth(dep):\n",
    "    for i in range(len(dep)):\n",
    "        mean = np.mean(dep[i].flatten())\n",
    "        std = np.std(dep[i].flatten())\n",
    "        dep[i] = (dep[i]-mean) /std\n",
    "    return dep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4e7b93-ea63-4641-8201-346b1d2b9fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LazyLoadDataset(Dataset):\n",
    "    \"\"\"\n",
    "    The point of using lazyloading is to save memory space, so you don't have\n",
    "    to store all of the data in train set to memory to start training\n",
    "    \"\"\"\n",
    "    def __init__(self, path, train=True, transform=None):\n",
    "        \"\"\"\n",
    "        1. initialize transformation\n",
    "        2. get path to X and Y for both train and test dataset\n",
    "        3. get the index list of the data set (0 to 3395)\n",
    "        \"\"\"\n",
    "        # initialize transform\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        \n",
    "        # setting up path to data X and labels Y\n",
    "        path = path + (\"train/\" if train else \"test/\")\n",
    "        self.pathX = path + \"X/\"\n",
    "        self.pathY = path + \"Y/\"\n",
    "        \n",
    "        # self.data stores the names of files as a list in train/X \n",
    "        # (and train/Y as well actually, since they are consistent)\n",
    "        # it's being used as a list of indices for finding data\n",
    "        # i.e. 0 to 3395\n",
    "        self.data = os.listdir(self.pathX)\n",
    "        #print(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get 1 instance in the dataset, containing 1 in X its corresponding Y\n",
    "        \n",
    "        idx: the index of the files (some number in 0 to 3395)\n",
    "        \"\"\"\n",
    "        # get that file\n",
    "        f = self.data[idx]\n",
    "        \n",
    "        # X\n",
    "        # read rgb images in the idx specified datapoint using cv2 \n",
    "        img0 = cv2.imread(self.pathX + f + \"/rgb/0.png\")\n",
    "        img1 = cv2.imread(self.pathX + f + \"/rgb/1.png\")\n",
    "        img2 = cv2.imread(self.pathX + f + \"/rgb/2.png\")\n",
    "        \n",
    "        # Apply the transform to all 3 images\n",
    "        # Normally, this contains some data format transformation & normalization\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        \n",
    "        # read depth images\n",
    "        depth = np.load(self.pathX + f + \"/depth.npy\")/1000\n",
    "        # normalize depth\n",
    "        depth = normalize_depth(depth)\n",
    "        \n",
    "        # read field_id\n",
    "        field_id = pkl.load(open(self.pathX + f + \"/field_id.pkl\", \"rb\"))\n",
    "        \n",
    "        # Y\n",
    "        # read labels\n",
    "        if self.train is True:\n",
    "            Y = np.load(self.pathY + f + \".npy\")\n",
    "        else: \n",
    "            Y = np.zeros(12)\n",
    "        \n",
    "        return (img0, img1, img2, depth, field_id), Y\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of data in the dataset (i.e. the length of self.data)\n",
    "        \"\"\"\n",
    "        return len(self.data)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a40762a6-2e61-40ac-aaf0-0a81d4550a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lazyload the train data with transformation to tensor and normalization\n",
    "# based on the standard score provided by ImageNet\n",
    "train_dataset = LazyLoadDataset(\"./lazydata/\", train=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "]))\n",
    "\n",
    "\"\"\"\n",
    "shuffling makes batches between epochs do not look alike.\n",
    "so for every epoch, it will give similar but different results\n",
    "\n",
    "img0:     torch.Size([64, 3, 224, 224])\n",
    "img1:     torch.Size([64, 3, 224, 224])\n",
    "img2:     torch.Size([64, 3, 224, 224])\n",
    "depth:    torch.Size([64, 3, 224, 224]) \n",
    "field_id: tuple of strings of length 64\n",
    "Y:        torch.Size([64, 12])\n",
    "\"\"\"\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "test_dataset= LazyLoadDataset(\"./lazydata/\", train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "]))\n",
    "\n",
    "\"\"\"\n",
    "shuffling makes batches between epochs do not look alike.\n",
    "so for every epoch, it will give similar but different results\n",
    "\n",
    "img0:     torch.Size([100, 3, 224, 224])\n",
    "img1:     torch.Size([100, 3, 224, 224])\n",
    "img2:     torch.Size([100, 3, 224, 224])\n",
    "depth:    torch.Size([100, 3, 224, 224]) \n",
    "field_id: tuple of strings of length 100\n",
    "Y:        torch.Size([100, 12])\n",
    "\"\"\"\n",
    "# \n",
    "# \n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f106f96-e97d-4c84-8889-5f708af9c398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3396\n",
      "849\n",
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([100, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# In total, 3396 train dataset pairs: (image, target output) = (3*224*224 matrix, torch.Size([12]) vector)\n",
    "print(len(train_dataset))\n",
    "# In total, 849 test dataset pairs: (image, target output) = (3*224*224 matrix, 12 vector)\n",
    "print(len(test_dataset))\n",
    "\n",
    "for (img0, img1, img2, depth, field_id), Y in train_loader:\n",
    "    print(img0.shape)\n",
    "    print(depth.shape)\n",
    "    \n",
    "    break\n",
    "\n",
    "for (img0, img1, img2, depth, field_id), Y  in test_loader:\n",
    "    print(depth.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "439d7c57-aa2f-475c-9418-f8e0a0de4b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e774cbb-c733-4656-9e9d-4812c4450858",
   "metadata": {},
   "source": [
    "# Try different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4b70e95-c115-46a1-9d76-c68ffe30d34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_img0_depth0(img, dep):\n",
    "    data = []\n",
    "    for i in range(len(img)):\n",
    "        image = torch.cat((img[i], dep[i][0].reshape(1,224,224))).reshape(1,4,224,224)\n",
    "        data.append(image)\n",
    "    return torch.cat(data)\n",
    "\n",
    "def merge_img1_depth1(img, dep):\n",
    "    data = []\n",
    "    for i in range(len(img)):\n",
    "        image = torch.cat((img[i], dep[i][1].reshape(1,224,224))).reshape(1,4,224,224)\n",
    "        data.append(image)\n",
    "    return torch.cat(data)\n",
    "\n",
    "def merge_img0_depth0_img1_depth1(img0,img1, dep):\n",
    "    data = []\n",
    "    for i in range(len(img0)):\n",
    "        image0 = torch.cat((img0[i], dep[i][0].reshape(1,224,224))).reshape(1,4,224,224)\n",
    "        image1 = torch.cat((img1[i], dep[i][1].reshape(1,224,224))).reshape(1,4,224,224)\n",
    "        image = torch.cat([image0, image1])\n",
    "        data.append(image)\n",
    " \n",
    "    return torch.cat(data)\n",
    "\n",
    "def merge_img0_img1(img0,img1):\n",
    "    data = []\n",
    "    for i in range(len(img0)):\n",
    "        image = torch.cat((img0[i].reshape(1,3,224,224), img1[i].reshape(1,3,224,224)))\n",
    "        data.append(image)\n",
    " \n",
    "    return torch.cat(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc089db-5b74-48a4-8571-26f5504df610",
   "metadata": {},
   "source": [
    "Q: It seems like in each data point X-rgb-2.png and X-depth[2] is not consistent with each other, so probably we should not consider using it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffccc2d6-1c5e-4446-9eee-ba9f103a5aaf",
   "metadata": {},
   "source": [
    "create dataset with RGBD 4 channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be53dd54-89f2-4050-8dbe-772dabaabe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, optimizer):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch\n",
    "\n",
    "    Args:\n",
    "        epoch (int): current epoch \n",
    "            An epoch means training the neural network with all the training data for one cycle. \n",
    "            In an epoch, we use all of the data exactly once. \n",
    "        model (nn.Module): model to train\n",
    "        optimizer (torch.optim): optimizer to use\n",
    "        permute_pixels (function): function to permute the pixels (default: None)\n",
    "        permutation_order (1D torch array): order of the permutation (default: None)\n",
    "    \"\"\"\n",
    "    model.train()  # Sets the model in training mode.\n",
    "    \n",
    "    for batch_idx, ((img0, img1, img2, depth, field_id), target) in enumerate(train_loader):\n",
    "        \"\"\"\n",
    "        batch_idx: index of batches in train_loader. total dataset = 3396, 64 dataset every batch\n",
    "                   3396/64 = 54 total number of batches\n",
    "                   \n",
    "        data:      torch.Size([64, 3, 224, 224]) -> 64 images, 3 channel/image, 224*224 pixels/image \n",
    "                OR torch.Size([64, 4, 224, 224])\n",
    "                OR torch.Size([128, 4, 224, 224])\n",
    "        target:    torch.Size([64,12]) -> 64 target output, ndarray of length 12, corresponding to 64 images\n",
    "                OR torch.Size([128,12])\n",
    "        \"\"\"\n",
    "        # # img0 as input data\n",
    "        data = img0\n",
    "        #data = depth\n",
    "        #data = merge_img0_img1(img0,img1)\n",
    "\n",
    "        #print(data.shape)\n",
    "        #target = torch.cat((target,target)).float()\n",
    "        target = target.float()\n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # zero out the old gradient; otherwise optimizer is gonna accumulate all the old gradients in itself\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute the output generated by the model\n",
    "        output = model.forward(data)\n",
    "        \n",
    "        # compute the value of the loss function\n",
    "        loss = torch.sqrt(F.mse_loss(output, target))\n",
    "        \n",
    "        # this computes the gradient of the loss function for every trainable parameter\n",
    "        # and the computed gradients are stored by the tensors themselves\n",
    "        loss.backward()\n",
    "        \n",
    "        # calling optimizer.step() makes the optimizer iterate over all parameters (tensors) it  \n",
    "        # is supposed to update and use their internally stored grad to update their values.\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(batch_idx)\n",
    "        # prints out the training status constantly to get a sense of how it is doing\n",
    "        if batch_idx % 1 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * train_batch_size, len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model):\n",
    "    \"\"\"\n",
    "    Test the model\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): model to test\n",
    "        permute (function): function to permute the pixels (default: None)\n",
    "        permutation_order (1D torch array): order of the permutation (default: None)\n",
    "    \"\"\"\n",
    "    model.eval()  # Sets the model in evaluation mode.\n",
    "    \n",
    "    pred = {}\n",
    "    for (img0, img1, img2, depth, field_id), target in test_loader:\n",
    "        \"\"\"\n",
    "        data:   torch.Size([1000, 1, 28, 28]) -> 1000 images, 1 channel/image, 28*28 pixels/image \n",
    "        target: torch.Size([1000]) -> 1000 target output, (0-9), corresponding to 1000 images\n",
    "        \"\"\"\n",
    "        # # img0 as input data\n",
    "        data = img0\n",
    "        #data = depth\n",
    "        #data = merge_img0_img1(img0,img1)\n",
    "        \n",
    "        # send to device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        \n",
    "        # compute the output generated by the model\n",
    "        # output: torch.Size([1000, 10])\n",
    "        output = model.forward(data)\n",
    "        for i in range(len(field_id)):\n",
    "            pred[field_id[i]] = output[i].data\n",
    "        #print(output.shape)\n",
    "    \n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d2f06-a217-4348-99bc-932f010ca83a",
   "metadata": {},
   "source": [
    "# Write a function for computing the total parameter count of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b6cb36c-04a5-455d-86fe-0dcd54781fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    \"\"\"\n",
    "    return number of parameters in model\n",
    "    \n",
    "    Add up the number of all the trainable parameters (p.requires_grad = True)\n",
    "    i.e. total # of weights and bias terms\n",
    "    \"\"\"\n",
    "    return sum(np.prod(p.shape) for p in model.parameters() if p.requires_grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a434942b-6dc2-46d0-b2f5-29c038f1d202",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train a small fully-connected network\n",
    "\n",
    "Optimizer : SGD with lr=0.01 and momentum=0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf01ca2-218d-4c9c-921c-0a757f314c7d",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dd0443a-9309-490c-adb2-df47d4617b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1(nn.Module):\n",
    "    def __init__(self, input_size, conv_feature, fc_feature, output_size):\n",
    "        super(CNN_1, self).__init__()\n",
    "        \n",
    "        \n",
    "        # Activation Function (e.g. sigmoid, ReLU)\n",
    "        self.relu = nn.ReLU()  \n",
    "        \n",
    "        #### CONVOLUTION ########\n",
    "        # Layer1: Convolution with 5*5 kernel\n",
    "        # (224*224, input_size channel) image -->  (220*220, conv_feature channels) feature maps\n",
    "        self.conv1 = nn.Conv2d(input_size, conv_feature, kernel_size=5)\n",
    "        \n",
    "        # Layer2: Pooling with 2*2 max pooling window\n",
    "        # (220*220, conv_feature channels) feature maps --> (110*110, conv_feature channels) feature maps\n",
    "        \n",
    "        # Layer3: Convolution with 5*5 kernel\n",
    "        # (110*110, conv_feature channels) feature maps --> (106*106, conv_feature channels) feature maps\n",
    "        self.conv2 = nn.Conv2d(conv_feature, conv_feature, kernel_size=5)\n",
    "        \n",
    "        # Layer4: Pooling with 2*2 max pooling window\n",
    "        # (106*106, conv_feature channels) feature maps --> (53*53, conv_feature channels) feature maps\n",
    "        \n",
    "        #### FCN ###############\n",
    "        # Flattens a (53*53, conv_feature channels) feature maps to a 53*53*conv_feature array\n",
    "        self.flatten = nn.Flatten()  \n",
    "        \n",
    "        # Layer5: Linear -- setting up Weight Matrix and Bias Vector\n",
    "        # (53*53, conv_feature channels) feature maps --> 53*53*conv_feature neurons --> fc_feature neurons\n",
    "        self.linear1 = nn.Linear(conv_feature*53*53, fc_feature)\n",
    "        \n",
    "        # Layer6: Linear\n",
    "        # fc_feature neurons --> output_size neurons\n",
    "        self.linear2 = nn.Linear(fc_feature, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input data that has the form (# of dataset, # of channels, 28, 28) -> torch.Size([64, 1, 28, 28])\n",
    "        \"\"\"\n",
    "        #print(x.shape)\n",
    "        # Layer1 operation: Convolution with 5*5 kernel\n",
    "        x = self.relu(self.conv1(x))\n",
    "        \n",
    "        # Layer2 operation: Pooling with 2*2 max pooling window\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Layer3 operation: Convolution with 5*5 kernel\n",
    "        x = self.relu(self.conv2(x))\n",
    "        \n",
    "        # Layer4 operation: Pooling with 2*2 max pooling window\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        #print(x.shape)\n",
    "        # Transform the input data to NeuralNetwork readable data\n",
    "        x = self.flatten(x)\n",
    "        #print(x.shape)\n",
    "        # Layer5 operation: 4*4*conv_feature neurons --> fc_feature neurons\n",
    "        x = self.relu(self.linear1(x))\n",
    "        \n",
    "        # Layer6 operation: fc_feature neurons --> output_size neurons\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # Apply the LogSoftMax() function on the last layer\n",
    "        #results = F.log_softmax(x, dim=1)\n",
    "        results = x\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d1ca7-a9b2-4949-a9f0-6b92ab2018d5",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f599bd61-4513-42e6-a897-e6d3163004b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_2(nn.Module):\n",
    "    def __init__(self, input_size, conv_feature, fc_feature, output_size):\n",
    "        super(CNN_2, self).__init__()\n",
    "        \n",
    "        \n",
    "        # Activation Function (e.g. sigmoid, ReLU)\n",
    "        self.relu = nn.ReLU()  \n",
    "        \n",
    "        #### CONVOLUTION ########\n",
    "        # Layer1: Convolution with 5*5 kernel\n",
    "        # (224*224, input_size channel) image -->  (220*220, conv_feature channels) feature maps\n",
    "        self.conv1 = nn.Conv2d(input_size, conv_feature, kernel_size=5)\n",
    "        \n",
    "        # Layer2: Pooling with 2*2 max pooling window\n",
    "        # (220*220, conv_feature channels) feature maps --> (110*110, conv_feature channels) feature maps\n",
    "        \n",
    "        # Layer3: Convolution with 5*5 kernel\n",
    "        # (110*110, conv_feature channels) feature maps --> (106*106, conv_feature channels) feature maps\n",
    "        self.conv2 = nn.Conv2d(conv_feature, conv_feature, kernel_size=5)\n",
    "        \n",
    "        # Layer4: Pooling with 2*2 max pooling window\n",
    "        # (106*106, conv_feature channels) feature maps --> (53*53, conv_feature channels) feature maps\n",
    "        \n",
    "        # Layer5: Convolution with 5*5 kernel\n",
    "        # (53*53, conv_feature channels) feature maps --> (48*48, conv_feature channels) feature maps\n",
    "        self.conv3 = nn.Conv2d(conv_feature, conv_feature, kernel_size=6)\n",
    "        \n",
    "        # Layer6: Pooling with 2*2 max pooling window\n",
    "        # (48*48, conv_feature channels) feature maps --> (24*24, conv_feature channels) feature maps\n",
    "        \n",
    "        #### FCN ###############\n",
    "        # Flattens a (24*24, conv_feature channels) feature maps to a 24*24*conv_feature array\n",
    "        self.flatten = nn.Flatten()  \n",
    "        \n",
    "        # Layer7: Linear -- setting up Weight Matrix and Bias Vector\n",
    "        # (24*24, conv_feature channels) feature maps --> 24*24*conv_feature neurons --> conv_feature*12*12\n",
    "        self.linear1 = nn.Linear(conv_feature*24*24, conv_feature*12*12)\n",
    "        \n",
    "        # Layer8: Linear -- setting up Weight Matrix and Bias Vector\n",
    "        # 12*12*conv_feature neurons --> conv_feature*6*6\n",
    "        self.linear2 = nn.Linear(conv_feature*12*12, conv_feature*6*6)\n",
    "        \n",
    "        # Layer9: Linear -- setting up Weight Matrix and Bias Vector\n",
    "        # 6*6*conv_feature neurons --> fc_feature=50\n",
    "        self.linear3 = nn.Linear(conv_feature*6*6, fc_feature)\n",
    "        \n",
    "        # Layer10: Linear\n",
    "        # fc_feature=50 neurons --> output_size neurons\n",
    "        self.linear4 = nn.Linear(fc_feature, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input data that has the form (# of dataset, # of channels, 28, 28) -> torch.Size([64, 1, 28, 28])\n",
    "        \"\"\"\n",
    "        #print(x.shape)\n",
    "        # Layer1 operation: Convolution with 5*5 kernel\n",
    "        x = self.relu(self.conv1(x))\n",
    "        \n",
    "        # Layer2 operation: Pooling with 2*2 max pooling window\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Layer3 operation: Convolution with 5*5 kernel\n",
    "        x = self.relu(self.conv2(x))\n",
    "        \n",
    "        # Layer4 operation: Pooling with 2*2 max pooling window\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Layer5 operation: Convolution with 6*6 kernel\n",
    "        x = self.relu(self.conv3(x))\n",
    "        \n",
    "        # Layer6 operation: Pooling with 2*2 max pooling window\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Transform the input data to NeuralNetwork readable data\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Layer7 operation: 24*24*conv_feature neurons --> conv_feature*12*12\n",
    "        x = self.relu(self.linear1(x))\n",
    "        \n",
    "        # Layer8 operation: conv_feature*12*12 --> conv_feature*6*6\n",
    "        x = self.relu(self.linear2(x))\n",
    "        \n",
    "        # Layer9 operation: 24*24*conv_feature neurons --> conv_feature*12*12\n",
    "        x = self.relu(self.linear3(x))\n",
    "        \n",
    "        # Layer10 operation: fc_feature neurons --> output_size neurons\n",
    "        x = self.linear4(x)\n",
    "        \n",
    "        # Apply the LogSoftMax() function on the last layer\n",
    "        #results = F.log_softmax(x, dim=1)\n",
    "        results = x\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4c9dde-d72d-4378-8c77-7a28f7533810",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d7f383f-db74-47cf-a86f-94af52b8a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_3(nn.Module):\n",
    "    def __init__(self, input_size, conv_feature, fc_feature, output_size):\n",
    "        super(CNN_3, self).__init__()\n",
    "        \n",
    "        \n",
    "        # Activation Function (e.g. sigmoid, ReLU)\n",
    "        self.relu = nn.ReLU()  \n",
    "        \n",
    "        #### CONVOLUTION ########\n",
    "        # Layer1: Convolution with 5*5 kernel\n",
    "        # (224*224, input_size channel) image -->  (220*220, conv_feature channels) feature maps\n",
    "        self.conv1 = nn.Conv2d(input_size, conv_feature, kernel_size=5)\n",
    "        \n",
    "        # Layer2: Pooling with 2*2 max pooling window\n",
    "        # (220*220, conv_feature channels) feature maps --> (110*110, conv_feature channels) feature maps\n",
    "        \n",
    "        # Layer3: Convolution with 5*5 kernel\n",
    "        # (110*110, conv_feature channels) feature maps --> (106*106, conv_feature channels) feature maps\n",
    "        self.conv2 = nn.Conv2d(conv_feature, conv_feature, kernel_size=5)\n",
    "        \n",
    "        # Layer4: Pooling with 2*2 max pooling window\n",
    "        # (106*106, conv_feature channels) feature maps --> (53*53, conv_feature channels) feature maps\n",
    "        \n",
    "        # Layer5: Convolution with 5*5 kernel\n",
    "        # (53*53, conv_feature channels) feature maps --> (48*48, 16 channels) feature maps\n",
    "        self.conv3 = nn.Conv2d(conv_feature, 16, kernel_size=6)\n",
    "        \n",
    "        # Layer6: Pooling with 2*2 max pooling window\n",
    "        # (48*48, 16 channels) feature maps --> (24*24, 16 channels) feature maps\n",
    "        \n",
    "        # Layer7: Convolution with 5*5 kernel\n",
    "        # (24*24, 16 channels) feature maps --> (20*20, 24 channels) feature maps\n",
    "        self.conv4 = nn.Conv2d(16, 24, kernel_size=5)\n",
    "        \n",
    "        # Layer8: Pooling with 2*2 max pooling window\n",
    "        # (20*20, 16 channels) feature maps --> (10*10, 24 channels) feature maps\n",
    "        \n",
    "        #### FCN ###############\n",
    "        # Flattens a (10*10, 24 channels) feature maps to a 10*10*24 array\n",
    "        self.flatten = nn.Flatten()  \n",
    "        \n",
    "        # Layer7: Linear -- setting up Weight Matrix and Bias Vector\n",
    "        # (10*10, 24 channels) feature maps --> 10*10*24 neurons --> fc_feature\n",
    "        self.linear1 = nn.Linear(24*10*10, fc_feature)\n",
    "        \n",
    "        # Layer8: Linear\n",
    "        # fc_feature=50 neurons --> output_size neurons\n",
    "        self.linear2 = nn.Linear(fc_feature, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input data that has the form (# of dataset, # of channels, 28, 28) -> torch.Size([64, 1, 28, 28])\n",
    "        \"\"\"\n",
    "        #print(x.shape)\n",
    "        # Layer1 operation: Convolution with 5*5 kernel\n",
    "        x = self.relu(self.conv1(x))\n",
    "        \n",
    "        # Layer2 operation: Pooling with 2*2 max pooling window\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Layer3 operation: Convolution with 5*5 kernel\n",
    "        x = self.relu(self.conv2(x))\n",
    "        \n",
    "        # Layer4 operation: Pooling with 2*2 max pooling window\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Layer5 operation: Convolution with 6*6 kernel\n",
    "        x = self.relu(self.conv3(x))\n",
    "        \n",
    "        # Layer6 operation: Pooling with 2*2 max pooling window\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Layer7 operation: Convolution with 5*5 kernel\n",
    "        x = self.relu(self.conv4(x))\n",
    "        \n",
    "        # Layer8 operation: Pooling with 2*2 max pooling window\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Transform the input data to NeuralNetwork readable data\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Layer9 operation: 10*10*24 neurons --> fc_feature neurons\n",
    "        x = self.relu(self.linear1(x))\n",
    "\n",
    "        \n",
    "        # Layer10 operation: fc_feature neurons --> output_size neurons\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # Apply the LogSoftMax() function on the last layer\n",
    "        #results = F.log_softmax(x, dim=1)\n",
    "        results = x\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db3e7c8-07ce-4f80-830f-da000d0f9614",
   "metadata": {},
   "source": [
    "# 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a181702c-8085-4459-a7d1-949a0276804b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_4(nn.Module):\n",
    "    def __init__(self, input_size, conv_feature, fc_feature, output_size):\n",
    "        super(CNN_4, self).__init__()\n",
    "        \n",
    "        \n",
    "        # Activation Function (e.g. sigmoid, ReLU)\n",
    "        self.relu = nn.ReLU()  \n",
    "        \n",
    "        #### CONVOLUTION ########\n",
    "        # Layer1: Convolution with 5*5 kernel\n",
    "        # (224*224, input_size channel) image -->  (220*220, conv_feature channels) feature maps\n",
    "        self.conv1 = nn.Conv2d(input_size, conv_feature, kernel_size=5)\n",
    "        \n",
    "        # Layer2: Pooling with 2*2 max pooling window\n",
    "        # (220*220, conv_feature channels) feature maps --> (110*110, conv_feature channels) feature maps\n",
    "        \n",
    "        # Layer3: Convolution with 5*5 kernel\n",
    "        # (110*110, conv_feature channels) feature maps --> (106*106, conv_feature channels) feature maps\n",
    "        self.conv2 = nn.Conv2d(conv_feature, conv_feature, kernel_size=5)\n",
    "        \n",
    "        # Layer4: Pooling with 2*2 max pooling window\n",
    "        # (106*106, conv_feature channels) feature maps --> (53*53, conv_feature channels) feature maps\n",
    "        \n",
    "        # Layer5: Convolution with 5*5 kernel\n",
    "        # (53*53, conv_feature channels) feature maps --> (48*48, 16 channels) feature maps\n",
    "        self.conv3 = nn.Conv2d(conv_feature, 16, kernel_size=6)\n",
    "        \n",
    "        # Layer6: Pooling with 2*2 max pooling window\n",
    "        # (48*48, 16 channels) feature maps --> (24*24, 16 channels) feature maps\n",
    "        \n",
    "        #### FCN ###############\n",
    "        # Flattens a (24*24, 16 channels) feature maps to a 24*24*16 array\n",
    "        self.flatten = nn.Flatten()  \n",
    "        \n",
    "        # Layer7: Linear -- setting up Weight Matrix and Bias Vector\n",
    "        # (24*24, 16 channels) feature maps --> 24*24*16 neurons --> 16*12*12\n",
    "        self.linear1 = nn.Linear(16*24*24, fc_feature)\n",
    "        \n",
    "        # Layer10: Linear\n",
    "        # fc_feature=50 neurons --> output_size neurons\n",
    "        self.linear4 = nn.Linear(fc_feature, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: input data that has the form (# of dataset, # of channels, 28, 28) -> torch.Size([64, 1, 28, 28])\n",
    "        \"\"\"\n",
    "        #print(x.shape)\n",
    "        # Layer1 operation: Convolution with 5*5 kernel\n",
    "        x = self.relu(self.conv1(x))\n",
    "        \n",
    "        # Layer2 operation: Pooling with 2*2 max pooling window\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Layer3 operation: Convolution with 5*5 kernel\n",
    "        x = self.relu(self.conv2(x))\n",
    "        \n",
    "        # Layer4 operation: Pooling with 2*2 max pooling window\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Layer5 operation: Convolution with 6*6 kernel\n",
    "        x = self.relu(self.conv3(x))\n",
    "        \n",
    "        # Layer6 operation: Pooling with 2*2 max pooling window\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        \n",
    "        # Transform the input data to NeuralNetwork readable data\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Layer7 operation: 24*24*conv_feature neurons --> conv_feature*12*12\n",
    "        x = self.relu(self.linear1(x))\n",
    "        \n",
    "        \n",
    "        # Layer10 operation: fc_feature neurons --> output_size neurons\n",
    "        x = self.linear4(x)\n",
    "        \n",
    "        # Apply the LogSoftMax() function on the last layer\n",
    "        #results = F.log_softmax(x, dim=1)\n",
    "        results = x\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add872ea-8da1-4292-817d-1b012a239d7e",
   "metadata": {},
   "source": [
    "### Train a ConvNet with the same number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dcad3c7-91a9-4396-8221-dd7a97cb0417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 210746792\n",
      "Train Epoch: 0 [0/3396 (0%)]\tLoss: 0.083879\n",
      "Train Epoch: 0 [64/3396 (2%)]\tLoss: 0.026983\n",
      "Train Epoch: 0 [128/3396 (4%)]\tLoss: 0.063400\n",
      "Train Epoch: 0 [192/3396 (6%)]\tLoss: 0.040996\n",
      "Train Epoch: 0 [256/3396 (7%)]\tLoss: 0.042409\n",
      "Train Epoch: 0 [320/3396 (9%)]\tLoss: 0.052392\n",
      "Train Epoch: 0 [384/3396 (11%)]\tLoss: 0.033465\n",
      "Train Epoch: 0 [448/3396 (13%)]\tLoss: 0.020518\n",
      "Train Epoch: 0 [512/3396 (15%)]\tLoss: 0.028862\n",
      "Train Epoch: 0 [576/3396 (17%)]\tLoss: 0.024200\n",
      "Train Epoch: 0 [640/3396 (19%)]\tLoss: 0.019523\n",
      "Train Epoch: 0 [704/3396 (20%)]\tLoss: 0.020073\n",
      "Train Epoch: 0 [768/3396 (22%)]\tLoss: 0.021086\n",
      "Train Epoch: 0 [832/3396 (24%)]\tLoss: 0.020613\n",
      "Train Epoch: 0 [896/3396 (26%)]\tLoss: 0.018675\n",
      "Train Epoch: 0 [960/3396 (28%)]\tLoss: 0.014097\n",
      "Train Epoch: 0 [1024/3396 (30%)]\tLoss: 0.014701\n",
      "Train Epoch: 0 [1088/3396 (31%)]\tLoss: 0.015647\n",
      "Train Epoch: 0 [1152/3396 (33%)]\tLoss: 0.016345\n",
      "Train Epoch: 0 [1216/3396 (35%)]\tLoss: 0.013896\n",
      "Train Epoch: 0 [1280/3396 (37%)]\tLoss: 0.013250\n",
      "Train Epoch: 0 [1344/3396 (39%)]\tLoss: 0.012263\n",
      "Train Epoch: 0 [1408/3396 (41%)]\tLoss: 0.012667\n",
      "Train Epoch: 0 [1472/3396 (43%)]\tLoss: 0.013389\n",
      "Train Epoch: 0 [1536/3396 (44%)]\tLoss: 0.012552\n",
      "Train Epoch: 0 [1600/3396 (46%)]\tLoss: 0.010342\n",
      "Train Epoch: 0 [1664/3396 (48%)]\tLoss: 0.010747\n",
      "Train Epoch: 0 [1728/3396 (50%)]\tLoss: 0.010131\n",
      "Train Epoch: 0 [1792/3396 (52%)]\tLoss: 0.010892\n",
      "Train Epoch: 0 [1856/3396 (54%)]\tLoss: 0.009668\n",
      "Train Epoch: 0 [1920/3396 (56%)]\tLoss: 0.009993\n",
      "Train Epoch: 0 [1984/3396 (57%)]\tLoss: 0.010075\n",
      "Train Epoch: 0 [2048/3396 (59%)]\tLoss: 0.010159\n",
      "Train Epoch: 0 [2112/3396 (61%)]\tLoss: 0.009450\n",
      "Train Epoch: 0 [2176/3396 (63%)]\tLoss: 0.009766\n",
      "Train Epoch: 0 [2240/3396 (65%)]\tLoss: 0.008458\n",
      "Train Epoch: 0 [2304/3396 (67%)]\tLoss: 0.009169\n",
      "Train Epoch: 0 [2368/3396 (69%)]\tLoss: 0.009431\n",
      "Train Epoch: 0 [2432/3396 (70%)]\tLoss: 0.009245\n",
      "Train Epoch: 0 [2496/3396 (72%)]\tLoss: 0.009251\n",
      "Train Epoch: 0 [2560/3396 (74%)]\tLoss: 0.009227\n",
      "Train Epoch: 0 [2624/3396 (76%)]\tLoss: 0.008580\n",
      "Train Epoch: 0 [2688/3396 (78%)]\tLoss: 0.008993\n",
      "Train Epoch: 0 [2752/3396 (80%)]\tLoss: 0.008491\n",
      "Train Epoch: 0 [2816/3396 (81%)]\tLoss: 0.009346\n",
      "Train Epoch: 0 [2880/3396 (83%)]\tLoss: 0.009734\n",
      "Train Epoch: 0 [2944/3396 (85%)]\tLoss: 0.007857\n",
      "Train Epoch: 0 [3008/3396 (87%)]\tLoss: 0.008441\n",
      "Train Epoch: 0 [3072/3396 (89%)]\tLoss: 0.007916\n",
      "Train Epoch: 0 [3136/3396 (91%)]\tLoss: 0.008359\n",
      "Train Epoch: 0 [3200/3396 (93%)]\tLoss: 0.007686\n",
      "Train Epoch: 0 [3264/3396 (94%)]\tLoss: 0.007667\n",
      "Train Epoch: 0 [3328/3396 (96%)]\tLoss: 0.008149\n",
      "Train Epoch: 0 [3392/3396 (98%)]\tLoss: 0.005395\n"
     ]
    }
   ],
   "source": [
    "# Training settings \n",
    "# input_size_cnn = 3 # number of channels\n",
    "# input_size_cnn = 4 # number of channels\n",
    "input_size_cnn = 3 # number of channels\n",
    "conv_features = 15 # number of feature maps\n",
    "fc_features = 5000\n",
    "output_size = 12\n",
    "\n",
    "model_cnn = CNN_1(input_size_cnn, conv_features, fc_features, output_size) # create CNN model\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.007, momentum=0.92)  # use SGD with learning rate 0.01 and momentum 0.5\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "test_loss = []\n",
    "for epoch in range(0, 1):\n",
    "   \n",
    "    train(epoch, model_cnn, optimizer)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4a8f6-a9b7-427e-b269-c5af12074253",
   "metadata": {},
   "source": [
    "# Predict Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c75268b-fdcb-4150-8f09-16dec9d2ae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred  = test(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60e1ba-38fc-4b56-9168-c659b51ea741",
   "metadata": {},
   "source": [
    "# Output csv file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d34bec38-051f-4bee-9210-72c987effc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ID  FINGER_POS_1  FINGER_POS_2  FINGER_POS_3  FINGER_POS_4  \\\n",
      "0    3596      0.049071      0.049423      0.125850      0.094789   \n",
      "1     464      0.034041      0.048613      0.125433      0.023360   \n",
      "2    3315      0.075615      0.043187      0.106857      0.097463   \n",
      "3     180      0.027825      0.054264      0.128523      0.037525   \n",
      "4    3488      0.069998      0.045921      0.106370      0.069000   \n",
      "..    ...           ...           ...           ...           ...   \n",
      "844  2408      0.080638      0.053840      0.096375      0.086239   \n",
      "845  1437      0.068833      0.049884      0.119150      0.111676   \n",
      "846  3189      0.035492      0.051378      0.127081      0.049323   \n",
      "847  3258      0.068011      0.054254      0.084629      0.082881   \n",
      "848   600      0.031115      0.052001      0.124860      0.057759   \n",
      "\n",
      "     FINGER_POS_5  FINGER_POS_6  FINGER_POS_7  FINGER_POS_8  FINGER_POS_9  \\\n",
      "0        0.003020      0.083745      0.084146     -0.033510     -0.012411   \n",
      "1       -0.003939      0.131314      0.048002     -0.056908      0.116470   \n",
      "2        0.013902      0.058942      0.067075     -0.031238     -0.003868   \n",
      "3       -0.006421      0.111909      0.063655     -0.036220      0.038211   \n",
      "4       -0.008573      0.097550      0.082048     -0.046200      0.097693   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "844      0.001343      0.047495      0.086017     -0.034995      0.055635   \n",
      "845     -0.005232      0.046760      0.068946     -0.046001     -0.044421   \n",
      "846     -0.006785      0.115761      0.067439     -0.041672      0.044193   \n",
      "847      0.009862      0.042886      0.087481     -0.034452      0.009976   \n",
      "848     -0.004648      0.112812      0.073781     -0.041344      0.025222   \n",
      "\n",
      "     FINGER_POS_10  FINGER_POS_11  FINGER_POS_12  \n",
      "0         0.064004       0.026865      -0.009799  \n",
      "1         0.048930       0.024145      -0.049971  \n",
      "2         0.060730       0.042857      -0.016870  \n",
      "3         0.053160       0.020314      -0.064422  \n",
      "4         0.048641       0.035337      -0.051856  \n",
      "..             ...            ...            ...  \n",
      "844       0.052254       0.063290      -0.021741  \n",
      "845       0.075035       0.051203      -0.017785  \n",
      "846       0.050464       0.025710      -0.049747  \n",
      "847       0.041709       0.044745       0.005687  \n",
      "848       0.058946       0.020756      -0.039972  \n",
      "\n",
      "[849 rows x 13 columns]\n",
      "Written to csv file submission16.csv\n"
     ]
    }
   ],
   "source": [
    "# Prints out the final results: \n",
    "#print(pred)\n",
    "\n",
    "outfile = 'submission16.csv'\n",
    "\n",
    "output_file = open(outfile, 'w')\n",
    "\n",
    "titles = ['ID', 'FINGER_POS_1', 'FINGER_POS_2', 'FINGER_POS_3', 'FINGER_POS_4', 'FINGER_POS_5', 'FINGER_POS_6',\n",
    "         'FINGER_POS_7', 'FINGER_POS_8', 'FINGER_POS_9', 'FINGER_POS_10', 'FINGER_POS_11', 'FINGER_POS_12']\n",
    "preds = []\n",
    "for key, item in pred.items():\n",
    "    lis = []\n",
    "    lis.append(key)\n",
    "    lis.extend(list(np.array(item)))\n",
    "    preds.append(lis)\n",
    "\n",
    "df = pd.DataFrame(preds)\n",
    "df.columns = titles\n",
    "print(df)\n",
    "df.to_csv(outfile, index = False)\n",
    "print(\"Written to csv file {}\".format(outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e64b254-4de6-48b6-929d-e53f50b8d475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868268e6-831d-45c2-9566-0a31076acf5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291ffbe-c8f0-43ff-bf07-7049d1dcf973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64e5695-ff32-488e-aaff-c26a8f5abfd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5453352-f258-49ea-8010-13417cae27ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe313f-7245-4549-a735-d425ed8b6f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
